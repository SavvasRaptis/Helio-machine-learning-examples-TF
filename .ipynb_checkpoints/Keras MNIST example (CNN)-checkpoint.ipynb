{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pretty-accordance",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "## Importing libraries\n",
    "\n",
    "As a first step we load the different libraries we are going to use, in this simple example we only need tensorflow (keras) and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "athletic-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import BatchNormalization,Dropout,Flatten, MaxPooling2D, Conv2D\n",
    "from tensorflow.keras.layers import LeakyReLU,PReLU,ELU,ThresholdedReLU,ReLU\n",
    "\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-married",
   "metadata": {},
   "source": [
    "## Data related parameters\n",
    "\n",
    "We define a few parameters related to the data we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "twenty-marketing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of the model \n",
    "num_classes = 10 # number of output class (1-9)\n",
    "input_shape = (28, 28, 1) # shape of the input, we have 28 x 28 pixel size images of each number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-explanation",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "Next step is to actually load the MNIST database and perform some simple pre-process in order to introduce it to the neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-charter",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() # we load the data from keras.datsets library\n",
    "\n",
    "# This automatically generates the train & test set (otherwise we could have done it manually or through another library)\n",
    "\n",
    "## Normalization ##\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255 # images are encoded with up to 256 so to normalize from 0-1 we simply divide\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-complement",
   "metadata": {},
   "source": [
    "## Model related parameters\n",
    "\n",
    "Here we define hyperparameters that are going to be used in the model and training below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5 # Number of epochs, where 1 epoch = 1 use of the training set\n",
    "n_batch_size = 128 # how many inputs are going to be used at the same time during one epoch. \n",
    "# note: small batch size can make the training faster but can make the generalization harder to achieve (less variety in sample)\n",
    "\n",
    "neurons_conv2d1 = 32\n",
    "kernel_size_conv2d1 = (3,3)\n",
    "neurons_conv2d1 = 64\n",
    "kernel_size_conv2d1 = (3,3)\n",
    "default_pool_size = (2,2)\n",
    "\n",
    "# number of neurons for the 1 hidden layers\n",
    "neurons_l1 = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "inside-shadow",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5c8f20d80aea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(Conv2D(neurons_conv2d1,kernel_size=kernel_size_conv2d1, activation =\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=default_pool_size))\n",
    "model.add(Conv2D(neurons_conv2d2,kernel_size_conv2d2, activation =\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=default_pool_size))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(neurons_l1))\n",
    "model.add(ReLU())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes,activation='softmax'))\n",
    "model.compile(\n",
    "              optimizer=\"adam\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-square",
   "metadata": {},
   "source": [
    "## Example of Training proccedure\n",
    "\n",
    "\n",
    "Let's now train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-assurance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 63s 1ms/sample - loss: 0.3065 - accuracy: 0.9061\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 60s 992us/sample - loss: 0.1008 - accuracy: 0.9689\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 59s 982us/sample - loss: 0.0768 - accuracy: 0.9770\n",
      "Epoch 4/5\n",
      "31488/60000 [==============>...............] - ETA: 24s - loss: 0.0620 - accuracy: 0.9822"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "measured-asset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.037109073073766194\n",
      "Test accuracy: 0.9862\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-kernel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
