{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf30b9ed-d763-4f9b-9a11-d067d34ebbec",
   "metadata": {},
   "source": [
    "## Original Publication\n",
    "\n",
    "See the original publication of this work in the following link: https://www.swsc-journal.org/articles/swsc/abs/2021/01/swsc210024/swsc210024.html\n",
    "\n",
    "Aminalragia-Giamini, Sigiava, et al. \"Solar Energetic Particle Event occurrence prediction using Solar Flare Soft X-ray measurements and Machine Learning.\" Journal of Space Weather and Space Climate 11 (2021): 59.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a378b43",
   "metadata": {},
   "source": [
    "# Loading Data \n",
    "\n",
    "## Importing libraries and data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20016c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 10:56:21.801644: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import sys\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras \n",
    "import xgboost as xgb\n",
    "\n",
    "x_train = pd.read_pickle(\"x_train.pkl\")  \n",
    "x_test = pd.read_pickle(\"x_test.pkl\")  \n",
    "y_train = pd.read_pickle(\"y_train.pkl\")  \n",
    "y_test = pd.read_pickle(\"y_test.pkl\")  \n",
    "\n",
    "# number of different classes\n",
    "n_SEPS =  226\n",
    "n_flares = 17875\n",
    "n_sample = n_SEPS +n_flares\n",
    "columns_to_use = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','17','20','21','22','28','39','40',\n",
    "                       '41','42','43','44','45','46','47','48']\n",
    "number_of_classes = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae342553-12a1-45ca-b0a0-36e920a330e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vs/s8nbsg4x7nd44x9k_00v8f0m0000gq/T/ipykernel_30406/4287163944.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  x_total = x_train.append(x_test)\n",
      "/var/folders/vs/s8nbsg4x7nd44x9k_00v8f0m0000gq/T/ipykernel_30406/4287163944.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  y_total = y_train.append(y_test)\n"
     ]
    }
   ],
   "source": [
    "x_total = x_train.append(x_test)\n",
    "y_total = y_train.append(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e4783",
   "metadata": {},
   "source": [
    "## Prepare validation\n",
    "\n",
    "Establish validation method below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10706183-fa4d-46f1-8a6a-33bc4f67865e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:58:40] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:42] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:42] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:42] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:42] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:42] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:42] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:42] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:42] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:42] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:42] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:42] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:42] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:42] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:42] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:42] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:42] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[10:58:42] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from xgboost import cv\n",
    "\n",
    "sample_weight = compute_sample_weight(class_weight='balanced',y=y_total)\n",
    "d_train = xgb.DMatrix(x_total, label=y_total)\n",
    "\n",
    "params={\"objective\" : \"binary:logistic\", 'learning_rate' :0.1, 'max_depth':10, 'scale_pos_weight':1}\n",
    "\n",
    "xgb_cv = cv(dtrain=d_train, params =params, nfold=50, metrics =\"auc\", num_boost_round=50,as_pandas =True)\n",
    "\n",
    "bst=xgb.XGBClassifier(max_depth=10,learning_rate=0.1, n_estimators=100, objective='binary:logistic', scale_pos_weight=1,eval_metric='logloss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14ad755c-6d22-48e0-8483-b518395f4294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-auc-mean</th>\n",
       "      <th>train-auc-std</th>\n",
       "      <th>test-auc-mean</th>\n",
       "      <th>test-auc-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.893955</td>\n",
       "      <td>0.008803</td>\n",
       "      <td>0.865037</td>\n",
       "      <td>0.108233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.915679</td>\n",
       "      <td>0.017160</td>\n",
       "      <td>0.879621</td>\n",
       "      <td>0.105628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.922297</td>\n",
       "      <td>0.014869</td>\n",
       "      <td>0.884749</td>\n",
       "      <td>0.103581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.928629</td>\n",
       "      <td>0.012565</td>\n",
       "      <td>0.891464</td>\n",
       "      <td>0.101359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.930319</td>\n",
       "      <td>0.010802</td>\n",
       "      <td>0.899493</td>\n",
       "      <td>0.091282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.931055</td>\n",
       "      <td>0.010517</td>\n",
       "      <td>0.907275</td>\n",
       "      <td>0.090747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.933298</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.909254</td>\n",
       "      <td>0.089689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.940896</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>0.909487</td>\n",
       "      <td>0.090356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.943678</td>\n",
       "      <td>0.009868</td>\n",
       "      <td>0.911373</td>\n",
       "      <td>0.089155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.946932</td>\n",
       "      <td>0.009272</td>\n",
       "      <td>0.922521</td>\n",
       "      <td>0.083234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.955943</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.923152</td>\n",
       "      <td>0.084156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.956718</td>\n",
       "      <td>0.007943</td>\n",
       "      <td>0.926003</td>\n",
       "      <td>0.082888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.957950</td>\n",
       "      <td>0.007448</td>\n",
       "      <td>0.929005</td>\n",
       "      <td>0.080729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.959713</td>\n",
       "      <td>0.006682</td>\n",
       "      <td>0.928889</td>\n",
       "      <td>0.080659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.961681</td>\n",
       "      <td>0.006304</td>\n",
       "      <td>0.933043</td>\n",
       "      <td>0.078859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.962918</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.932939</td>\n",
       "      <td>0.079044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.963980</td>\n",
       "      <td>0.005619</td>\n",
       "      <td>0.934747</td>\n",
       "      <td>0.078343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.964948</td>\n",
       "      <td>0.005867</td>\n",
       "      <td>0.936135</td>\n",
       "      <td>0.074675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.967152</td>\n",
       "      <td>0.005360</td>\n",
       "      <td>0.935682</td>\n",
       "      <td>0.075935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.968992</td>\n",
       "      <td>0.004766</td>\n",
       "      <td>0.937484</td>\n",
       "      <td>0.076056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.970316</td>\n",
       "      <td>0.003737</td>\n",
       "      <td>0.939675</td>\n",
       "      <td>0.075459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.971063</td>\n",
       "      <td>0.002874</td>\n",
       "      <td>0.939704</td>\n",
       "      <td>0.076258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.972360</td>\n",
       "      <td>0.002511</td>\n",
       "      <td>0.941360</td>\n",
       "      <td>0.075569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.975271</td>\n",
       "      <td>0.001932</td>\n",
       "      <td>0.944338</td>\n",
       "      <td>0.074882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.976004</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>0.944645</td>\n",
       "      <td>0.075053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.976596</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>0.944808</td>\n",
       "      <td>0.075042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.977028</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.945174</td>\n",
       "      <td>0.075392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.977454</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.944744</td>\n",
       "      <td>0.075679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.977858</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.944520</td>\n",
       "      <td>0.075708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.979466</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>0.943730</td>\n",
       "      <td>0.076526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.980908</td>\n",
       "      <td>0.001759</td>\n",
       "      <td>0.945444</td>\n",
       "      <td>0.073825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.981647</td>\n",
       "      <td>0.001467</td>\n",
       "      <td>0.945769</td>\n",
       "      <td>0.073405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.981915</td>\n",
       "      <td>0.001340</td>\n",
       "      <td>0.947225</td>\n",
       "      <td>0.073350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.982050</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>0.947367</td>\n",
       "      <td>0.073336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.982560</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.947284</td>\n",
       "      <td>0.073608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.983790</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.947254</td>\n",
       "      <td>0.073914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.984567</td>\n",
       "      <td>0.001586</td>\n",
       "      <td>0.947221</td>\n",
       "      <td>0.074253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.986152</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.946816</td>\n",
       "      <td>0.074508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.988538</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>0.947478</td>\n",
       "      <td>0.075284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.989048</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.947282</td>\n",
       "      <td>0.075935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.989381</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>0.948353</td>\n",
       "      <td>0.075440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.989644</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.948329</td>\n",
       "      <td>0.075748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.989902</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.952368</td>\n",
       "      <td>0.070669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.990077</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.952494</td>\n",
       "      <td>0.070607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.990218</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.954514</td>\n",
       "      <td>0.069329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.990350</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.954951</td>\n",
       "      <td>0.069326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.990570</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.954849</td>\n",
       "      <td>0.069385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.991403</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.956724</td>\n",
       "      <td>0.066175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.992773</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.956521</td>\n",
       "      <td>0.066272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.993036</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.957784</td>\n",
       "      <td>0.065409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
       "0         0.893955       0.008803       0.865037      0.108233\n",
       "1         0.915679       0.017160       0.879621      0.105628\n",
       "2         0.922297       0.014869       0.884749      0.103581\n",
       "3         0.928629       0.012565       0.891464      0.101359\n",
       "4         0.930319       0.010802       0.899493      0.091282\n",
       "5         0.931055       0.010517       0.907275      0.090747\n",
       "6         0.933298       0.010700       0.909254      0.089689\n",
       "7         0.940896       0.009004       0.909487      0.090356\n",
       "8         0.943678       0.009868       0.911373      0.089155\n",
       "9         0.946932       0.009272       0.922521      0.083234\n",
       "10        0.955943       0.008000       0.923152      0.084156\n",
       "11        0.956718       0.007943       0.926003      0.082888\n",
       "12        0.957950       0.007448       0.929005      0.080729\n",
       "13        0.959713       0.006682       0.928889      0.080659\n",
       "14        0.961681       0.006304       0.933043      0.078859\n",
       "15        0.962918       0.005650       0.932939      0.079044\n",
       "16        0.963980       0.005619       0.934747      0.078343\n",
       "17        0.964948       0.005867       0.936135      0.074675\n",
       "18        0.967152       0.005360       0.935682      0.075935\n",
       "19        0.968992       0.004766       0.937484      0.076056\n",
       "20        0.970316       0.003737       0.939675      0.075459\n",
       "21        0.971063       0.002874       0.939704      0.076258\n",
       "22        0.972360       0.002511       0.941360      0.075569\n",
       "23        0.975271       0.001932       0.944338      0.074882\n",
       "24        0.976004       0.001848       0.944645      0.075053\n",
       "25        0.976596       0.001765       0.944808      0.075042\n",
       "26        0.977028       0.001328       0.945174      0.075392\n",
       "27        0.977454       0.001351       0.944744      0.075679\n",
       "28        0.977858       0.001506       0.944520      0.075708\n",
       "29        0.979466       0.002234       0.943730      0.076526\n",
       "30        0.980908       0.001759       0.945444      0.073825\n",
       "31        0.981647       0.001467       0.945769      0.073405\n",
       "32        0.981915       0.001340       0.947225      0.073350\n",
       "33        0.982050       0.001357       0.947367      0.073336\n",
       "34        0.982560       0.001779       0.947284      0.073608\n",
       "35        0.983790       0.001772       0.947254      0.073914\n",
       "36        0.984567       0.001586       0.947221      0.074253\n",
       "37        0.986152       0.001615       0.946816      0.074508\n",
       "38        0.988538       0.001263       0.947478      0.075284\n",
       "39        0.989048       0.001078       0.947282      0.075935\n",
       "40        0.989381       0.001039       0.948353      0.075440\n",
       "41        0.989644       0.000994       0.948329      0.075748\n",
       "42        0.989902       0.000850       0.952368      0.070669\n",
       "43        0.990077       0.000843       0.952494      0.070607\n",
       "44        0.990218       0.000838       0.954514      0.069329\n",
       "45        0.990350       0.000831       0.954951      0.069326\n",
       "46        0.990570       0.000912       0.954849      0.069385\n",
       "47        0.991403       0.001080       0.956724      0.066175\n",
       "48        0.992773       0.001037       0.956521      0.066272\n",
       "49        0.993036       0.001085       0.957784      0.065409"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78432287",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example of Training proccedure\n",
    "\n",
    "\n",
    "Let's now train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eb12477",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst=xgb.XGBClassifier(max_depth=10,learning_rate=0.01, n_estimators=50, objective='binary:logistic')\n",
    "# fit model\n",
    "model_used=bst.fit(x_total, y_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f84e03",
   "metadata": {},
   "source": [
    "# Visualziating results (simple metrics)\n",
    "\n",
    "Let's see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3ae6e7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m confusion_matrix\n\u001b[1;32m      4\u001b[0m predictions \u001b[39m=\u001b[39m model_used\u001b[39m.\u001b[39mpredict(x_test)\n\u001b[0;32m----> 5\u001b[0m predictions_binary \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margmax(predictions, axis \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m y_test_binary \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(y_test, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m cr \u001b[39m=\u001b[39m classification_report(y_test_binary, predictions_binary)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:1216\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m-> 1216\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predictions = model_used.predict(x_test)\n",
    "predictions_binary = np.argmax(predictions, axis = 1)\n",
    "y_test_binary = np.argmax(y_test, axis=1)\n",
    "    \n",
    "cr = classification_report(y_test_binary, predictions_binary)\n",
    "cm = confusion_matrix(y_test_binary, predictions_binary)\n",
    "\n",
    "print(cm)\n",
    "print(\"Class 1 accuracy\")\n",
    "print(cm[0,0]/(cm[0,0]+cm[0,1]))\n",
    "\n",
    "print(\"Class 2 accuracy\")\n",
    "print(cm[1,1]/(cm[1,0]+cm[1,1]))\n",
    "\n",
    "#print(model_used.summary())\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f09fd1",
   "metadata": {},
   "source": [
    "# Visualziating results (Plots)\n",
    "\n",
    "Let's see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c37c5b56-65bb-4637-8dfd-801010b70c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00814376, 0.01327463, 0.01484632, 0.00787467, 0.02193077,\n",
       "       0.01172764, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.01498673, 0.00882444, 0.10886821, 0.03972897, 0.01323617,\n",
       "       0.00666374, 0.01276817, 0.01295464, 0.01944197, 0.00858052,\n",
       "       0.        , 0.        , 0.01888355, 0.01548385, 0.01333147,\n",
       "       0.00981255, 0.34151903, 0.06203757, 0.01205044, 0.01337198,\n",
       "       0.01716927, 0.01633044, 0.00681656, 0.01264735, 0.01296095,\n",
       "       0.01367681, 0.        , 0.        , 0.        , 0.01914397,\n",
       "       0.00777842, 0.00943802, 0.00512267, 0.01109561, 0.00663682,\n",
       "       0.01324036, 0.00990741, 0.01108752, 0.01660605], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21df82b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'XGBClassifier' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7576\\1324738466.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mval_f1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_used\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_f1_m'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmodel_used\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmodel_used\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'XGBClassifier' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "val_f1 = model_used.history['val_f1_m']\n",
    "val_loss= model_used.history['val_loss']\n",
    "val_acc= model_used.history['val_acc']\n",
    "val_prec= model_used.history['val_precision_m']\n",
    "\n",
    "train_f1 = model_used.history['f1_m']\n",
    "train_loss = model_used.history['loss']\n",
    "train_acc = model_used.history['acc']\n",
    "train_prec = model_used.history['precision_m']\n",
    "\n",
    "plt.figure(figsize=(11.69,8.27))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(train_f1)\n",
    "plt.plot(val_f1)\n",
    "plt.ylabel('f1 score',fontsize=22)\n",
    "plt.legend(['train', 'test'], loc='lower right',fontsize=16)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.ylabel('loss',fontsize=22)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(train_acc)\n",
    "plt.plot(val_acc)\n",
    "plt.xlabel('epochs',fontsize=22)\n",
    "plt.ylabel('acc',fontsize=22)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train_prec)\n",
    "plt.plot(val_prec)\n",
    "plt.xlabel('epochs',fontsize=22)\n",
    "plt.ylabel('precision',fontsize=22)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
