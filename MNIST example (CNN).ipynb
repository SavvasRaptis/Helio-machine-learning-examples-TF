{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "synthetic-sarah",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "## Importing libraries\n",
    "\n",
    "As a first step we load the different libraries we are going to use, in this simple example we only need tensorflow (keras) and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "athletic-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import BatchNormalization,Dropout,Flatten, MaxPooling2D, Conv2D\n",
    "from tensorflow.keras.layers import LeakyReLU,PReLU,ELU,ThresholdedReLU,ReLU\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-latter",
   "metadata": {},
   "source": [
    "## Data related parameters\n",
    "\n",
    "We define a few parameters related to the data we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "twenty-marketing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of the model \n",
    "num_classes = 10 # number of output class (1-9)\n",
    "input_shape = (28, 28, 1) # shape of the input, we have 28 x 28 pixel size images of each number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-period",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "Next step is to actually load the MNIST database and perform some simple pre-process in order to introduce it to the neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "first-hungary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() # we load the data from keras.datsets library\n",
    "\n",
    "# This automatically generates the train & test set (otherwise we could have done it manually or through another library)\n",
    "\n",
    "## Normalization ##\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255 # images are encoded with up to 256 so to normalize from 0-1 we simply divide\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-cookbook",
   "metadata": {},
   "source": [
    "## Model related parameters\n",
    "\n",
    "Here we define hyperparameters that are going to be used in the model and training below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "neither-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5 # Number of epochs, where 1 epoch = 1 use of the training set\n",
    "n_batch_size = 128 # how many inputs are going to be used at the same time during one epoch. \n",
    "# note: small batch size can make the training faster but can make the generalization harder to achieve (less variety in sample)\n",
    "\n",
    "neurons_conv2d1 = 32\n",
    "kernel_size_conv2d1 = (3,3)\n",
    "neurons_conv2d2 = 64\n",
    "kernel_size_conv2d2 = (3,3)\n",
    "default_pool_size = (2,2)\n",
    "\n",
    "# number of neurons for the 1 hidden layers\n",
    "neurons_l1 = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "personal-adaptation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(Conv2D(neurons_conv2d1,kernel_size=kernel_size_conv2d1, activation =\"relu\")) # obtaining features from image\n",
    "model.add(MaxPooling2D(pool_size=default_pool_size)) # reducing dimensionality\n",
    "model.add(Conv2D(neurons_conv2d2,kernel_size_conv2d2, activation =\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=default_pool_size))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten()) # Since the image is 2D we need to \"flatten\" it into a single array in order to be used as input\n",
    "model.add(Dense(neurons_l1))\n",
    "model.add(ReLU())\n",
    "model.add(Dropout(0.5))  # this layers protects the model from overfitting to the test data. Can we replace it maybe?\n",
    "model.add(Dense(num_classes,activation='softmax')) # multi-class classification problem : softmax activation\n",
    "model.compile(\n",
    "              optimizer=\"adam\", # any optimizer works well, adam is a standard choice for simple problems\n",
    "              loss=\"categorical_crossentropy\", # loss function for classification = crossentropy\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-square",
   "metadata": {},
   "source": [
    "## Example of Training proccedure\n",
    "\n",
    "\n",
    "Let's now train our model for a few epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "varied-assurance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 31s 66ms/step - loss: 0.3222 - accuracy: 0.9013\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 32s 68ms/step - loss: 0.1090 - accuracy: 0.9672\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 34s 72ms/step - loss: 0.0794 - accuracy: 0.9758\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 37s 78ms/step - loss: 0.0682 - accuracy: 0.9798\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 34s 72ms/step - loss: 0.0570 - accuracy: 0.9831\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=n_epochs, batch_size=n_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-symbol",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "To evulate our model we need to see how well it performs to unknown data (test set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "measured-asset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.02673148177564144\n",
      "Test accuracy: 0.9908000230789185\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "regional-kernel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 977    0    0    0    0    0    1    1    1    0]\n",
      " [   0 1133    1    0    0    0    1    0    0    0]\n",
      " [   2    1 1021    0    1    0    1    6    0    0]\n",
      " [   0    0    2 1000    0    4    0    3    1    0]\n",
      " [   0    1    0    0  971    0    3    0    2    5]\n",
      " [   2    0    0    4    0  883    1    1    0    1]\n",
      " [   1    2    0    0    1    2  952    0    0    0]\n",
      " [   0    3    3    0    0    0    0 1019    1    2]\n",
      " [   3    0    3    1    0    1    1    2  960    3]\n",
      " [   1    3    0    0    3    4    1    4    1  992]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       980\n",
      "           1       0.99      1.00      0.99      1135\n",
      "           2       0.99      0.99      0.99      1032\n",
      "           3       1.00      0.99      0.99      1010\n",
      "           4       0.99      0.99      0.99       982\n",
      "           5       0.99      0.99      0.99       892\n",
      "           6       0.99      0.99      0.99       958\n",
      "           7       0.98      0.99      0.99      1028\n",
      "           8       0.99      0.99      0.99       974\n",
      "           9       0.99      0.98      0.99      1009\n",
      "\n",
      "    accuracy                           0.99     10000\n",
      "   macro avg       0.99      0.99      0.99     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_classes(x_test)\n",
    "y_test_binary = np.argmax(y_test, axis=1)\n",
    "    \n",
    "cr = classification_report(y_test_binary, predictions)\n",
    "cm = confusion_matrix(y_test_binary, predictions)\n",
    "print(cm)\n",
    "print(cr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
